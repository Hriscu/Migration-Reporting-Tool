<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Technical Report - MiRT (Migration Reporting Tool)</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <h1>Technical Report - Migration Reporting Tool (MiRT)</h1>
        <p><strong>Authors:</strong> Hriscu Alexandru-Gabriel &amp; Iațu Antonio<br></p>
        <strong>Date:</strong> February 2025</p>
    </header>

    <nav>
        <ul>
            <li><a href="#introduction">Introduction</a></li>
            <li><a href="#data-structures">Internal Data Structures</a></li>
            <li><a href="#api-architecture">API Architecture</a></li>
            <li><a href="#rdf-models">RDF-Based Knowledge Models</a></li>
            <li><a href="#external-sources">Use of External Data Sources</a></li>
            <li><a href="#linked-data">Linked Data Conformance</a></li>
        </ul>
    </nav>

    <main>
        <section id="introduction">
            <h2>Introduction</h2>
			<h4>Overview of MiRT (Migration Reporting Tool)</h4>
			<p>The Migration Reporting Tool (MiRT) is an advanced web application aimed at helping users understand and analyze migration patterns of various entities, such as birds, humans, and extraterrestrial beings. The main objective of MiRT is to offer a secure, reliable, and user-friendly platform that provides comprehensive data to gain valuable insights into migration events. To improve user engagement and experience, MiRT features interactive map visualizations, data-driven statistics, and in-depth analysis of migration subtopics. Additionally, users can save maps and track migration trends, ensuring they have the tools needed to explore and interpret the complex dynamics of migration.
			</p>

			<h4>Technology Choices and Rationale</h4>
			
			<h5>Reddit API</h5>
			<p>One of the key decisions in the development of MiRT was the selection of the Reddit API for retrieving migration-related event data. After evaluating various data sources, we favored Reddit due to its easy-to-use API and the practical experience our team has with it. Reddit offers a wealth of discussions, posts, and comments that provide rich, real-time data about migration events. Its open access and simplicity for integration made it the ideal choice for gathering and analyzing migration data, helping us to quickly iterate on our application.</p>

			<h5>Heroku Platform</h5>
			For deployment, MiRT leverages Heroku. We chose Heroku due to our team's previous experience with the platform and its strong reputation for ease of use and user-friendliness. Heroku offers a streamlined deployment process, making it particularly beneficial for small to medium-sized projects. Its ability to scale and its seamless integration with various databases and add-ons made it a reliable option for ensuring the availability and stability of our application.

			<h5>Fuseki as the Triple Store</h5>
			<p>In terms of data management, we selected Fuseki as our SPARQL endpoint and triple store solution. This choice was primarily driven by Fuseki’s convenience and accessibility, which allows for efficient querying and storage of RDF data. As a versatile and lightweight solution, Fuseki integrates seamlessly with the other technologies used in MiRT, providing a robust platform for managing linked data related to migration events, species, and locations.</p>

			<h5>Google Cloud VM Instances</h5>
			<p>For hosting and computing resources, we chose Google Cloud VM Instances. This decision was based on Google Cloud’s reputation for reliability, security, and the opportunity to experiment with new technologies. By utilizing Google Cloud, we ensure that MiRT benefits from a secure and scalable environment, capable of handling data processing and storage requirements efficiently.</p>

			<h5>Django Backend Framework</h5>
			<p>On the backend, MiRT is powered by Django, a high-level Python web framework known for its robustness and solid architectural structure. Django allows for excellent separation of concerns, providing a clear distinction between the frontend and backend layers. This separation facilitates easier development and maintenance, while also ensuring that the backend can scale and perform efficiently. Django's features, such as its built-in authentication system and modularity, made it an ideal choice for building a secure and maintainable platform like MiRT.</p>
        </section>

		<section id="data-structures">
			<h2>Internal Data Structures</h2>
			<p>MiRT manages multiple data structures to handle migration events, user interactions, and metadata. The core models include:</p>
			<ul>
				<li><strong>TopicModeling:</strong> Aggregates data for generating statistics regarding comments on a given migration topic.</li>
				<li><strong>MongoDbClient.py:</strong> Responsible for setting up and maintaining the connection to MongoDB. It provides an interface for other components to interact with the database.</li>
				<li><strong>RedditComment.py:</strong> Defines the structure of Reddit comment objects, with methods to convert between objects and dictionaries, making it easier to interact with the database.</li>
				<li><strong>RedditPost.py:</strong> Defines the structure of Reddit post objects, with fields like title, content, location, and keywords. This class also includes methods for converting posts into dictionary form for storage.</li>
				<li><strong>RedditCommentsRepository.py:</strong> This class provides methods for querying, saving, and updating Reddit comments in the MongoDB database. It simplifies interactions with the Reddit comments collection.</li>
				<li><strong>RedditPostsRepository.py:</strong> Similar to the RedditCommentsRepository, this repository handles interactions with the Reddit posts collection in MongoDB.</li>
				<li><strong>RedditService.py:</strong> The service that connects to Reddit's API, fetches posts and comments, extracts relevant information (e.g., location, keywords), and saves the data in MongoDB. It is responsible for ensuring that all data is processed and stored correctly.</li>
			</ul>
			<h3>Data Collector API Overview</h3>
			<p>The <strong>Data Collector API</strong> is responsible for gathering, processing, and storing data from various Reddit posts and comments related to migration topics. It fetches data from selected subreddits like "birding," "aliens," and "IWantOut," processes this data (e.g., extracting location, keywords, and media), and saves it into MongoDB for later analysis.</p>

			<h4>Key Files and Their Roles</h4>
			<ul>
				<li><strong>DataPreprocessor.py:</strong> Contains methods to preprocess and clean the text data from Reddit posts and comments before saving them. This ensures that only meaningful and structured data is stored in the database.</li>
				<li><strong>MongoDbClient.py:</strong> Responsible for setting up and maintaining the connection to MongoDB. It provides an interface for other components to interact with the database.</li>
				<li><strong>RedditComment.py:</strong> Defines the structure of Reddit comment objects, with methods to convert between objects and dictionaries, making it easier to interact with the database.</li>
				<li><strong>RedditPost.py:</strong> Defines the structure of Reddit post objects, with fields like title, content, location, and keywords. This class also includes methods for converting posts into dictionary form for storage.</li>
				<li><strong>RedditCommentsRepository.py:</strong> This class provides methods for querying, saving, and updating Reddit comments in the MongoDB database. It simplifies interactions with the Reddit comments collection.</li>
				<li><strong>RedditPostsRepository.py:</strong> Similar to the RedditCommentsRepository, this repository handles interactions with the Reddit posts collection in MongoDB.</li>
				<li><strong>RedditService.py:</strong> The service that connects to Reddit's API, fetches posts and comments, extracts relevant information (e.g., location, keywords), and saves the data in MongoDB. It is responsible for ensuring that all data is processed and stored correctly.</li>
			</ul>

			<h4>Data Collection Flow</h4>
			<ol>
				<li><strong>Fetch Posts:</strong> The <strong>RedditService</strong> fetches posts from different subreddits such as "birding," "aliens," and "IWantOut." It can be extended to handle other subreddits.</li>
				<li><strong>Extract Data:</strong> For each post, the service extracts comments, processes the text, and extracts additional data such as keywords and location using the <strong>DataPreprocessor</strong> and <strong>RedditComment</strong> classes.</li>
				<li><strong>Store Data:</strong> The <strong>RedditPostsRepository</strong> and <strong>RedditCommentsRepository</strong> classes save posts and comments to MongoDB for later analysis.</li>
				<li><strong>Location Extraction:</strong> The <strong>RedditService</strong> uses a geolocation service (e.g., <strong>Geopy</strong>) to extract location coordinates from text, which are stored alongside the post and comment data.</li>
			</ol>

			<h4>Example of Data Flow</h4>
			<ol>
				<li>The service fetches a Reddit post from the "birding" subreddit.</li>
				<li>It then extracts comments from that post, cleans the text using the <strong>DataPreprocessor</strong> class, and identifies keywords such as "bird" and "migration."</li>
				<li>The service extracts the location mentioned in the post or comment using the <strong>RedditService</strong> and saves this information alongside the post and comment data.</li>
				<li>All data is stored in MongoDB, which can then be queried for analysis, reporting, or visualization.</li>
			</ol>

			<p>This structure ensures that the data collected from Reddit is clean, well-structured, and ready for further analysis, such as migration event tracking or user sentiment analysis.</p>

			<h3>Business Process Model (BPMN)</h3>
			<p>The following diagram illustrates the business process flow for data collection:</p>
			<img src="resources/BPMN_DataCollectorAPI.png" alt="Business Process BPMN Diagram" style="width: 80%; max-width: 800px; display: block; margin: 20px auto;">

			<h3>UML Diagram</h3>
			<p>The following UML diagram shows the system's design and class structure:</p>
			<img src="resources/UML_DataCollectorAPI.png" alt="UML Diagram" style="width: 80%; max-width: 800px; display: block; margin: 20px auto;">

			<h3>C4 Diagrams</h3>
			<h4>Context Diagram (Level 1)</h4>
			<p>This diagram shows the overall system and its interactions with external entities:</p>
			<img src="resources/C4_level_1_diagram.jpeg" alt="Context Diagram" style="width: 80%; max-width: 800px; display: block; margin: 20px auto;">

			<h4>Container Diagram (Level 2)</h4>
			<p>This diagram provides a detailed view of the system's containers and their responsibilities:</p>
			<img src="resources/C4_level_2_diagram.jpeg" alt="Container Diagram" style="width: 80%; max-width: 800px; display: block; margin: 20px auto;">

			<h4>Component Diagram (Level 3)</h4>
			<p>This diagram breaks down the containers into individual components, illustrating their interactions:</p>
			<img src="resources/C4_level_3_data_collector_diagram.jpeg" alt="Component Diagram" style="width: 80%; max-width: 800px; display: block; margin: 20px auto;">
			<!-- Topic Modeling Details -->
			<h3>Topic Modeling Details</h3>
			<p>The <strong>TopicModeling</strong> module is responsible for analyzing comments related to migration events. It uses Latent Dirichlet Allocation (LDA) to identify hidden topics and generate statistical insights.</p>

			<h4>Environment Setup</h4>
			<p>Install the necessary Python packages:</p>
			<pre><code>pip install pandas pyLDAvis gensim nltk spacy wordsegment contractions requests python-dotenv</code></pre>

			<p>Download required NLTK resources:</p>
			<pre><code>import nltk
		nltk.download('punkt')
		nltk.download('stopwords')
		nltk.download('wordnet')</code></pre>

			<p>For API integration, create a <code>.env</code> file inside the <code>TopicModeling</code> directory and add your API key from 
			<a href="https://aistudio.google.com/apikey" target="_blank">Google AI Studio</a>:</p>
			<pre><code>API_KEY=your_api_key_here</code></pre>

			<h4>Code Structure</h4>
			<ul>
				<li><strong>main.py:</strong> Initializes and runs the LDA topic modeling pipeline.</li>
				<li><strong>lda_model.py:</strong> Manages LDA model training and optimization.</li>
				<li><strong>preprocessor.py:</strong> Handles text cleaning, tokenization, and stopword removal.</li>
				<li><strong>get_topic_name.py:</strong> Uses the Google Gemini API to assign descriptive names to detected topics.</li>
				<li><strong>lda_visualisation_numTopicsNR.html:</strong> Provides interactive visualizations of topic distributions.</li>
				<li><strong>trainig_data_collector.py:</strong> Aggregates raw data from various sources into training datasets.</li>
				<li><strong>SavedModels/:</strong> Stores trained LDA models for future analysis.</li>
			</ul>
			
			<h4>Workflow Overview</h4>
			<ol>
				<li>Preprocess comments using <code>preprocessor.py</code>.</li>
				<li>Train the LDA model with <code>lda_model.py</code>.</li>
				<li>Generate topic names via <code>get_topic_name.py</code>.</li>
				<li>Visualize topic distributions with <code>lda_visualisation_numTopicsNR.html</code>.</li>
			</ol>

			<p>This structure helps MiRT provide deeper insights into migration-related discussions through automated topic discovery and analysis.</p>

			<h4>Backend</h4>
			
			<p>In addition to the data collector API, our backend offers a variety of essential services. Built with Django, the backend follows a microservices architecture, ensuring scalability and flexibility. Key services include:</p>

			<ul>
				<li><strong>mirt_back:</strong> This core service houses the fundamental components of the application. It acts as the central orchestrator, managing and directing which services are called and when.</li>
				<li><strong>DBpedia_things:</strong> This service enriches the database by fetching valuable information about birds and locations, enhancing the data we work with.</li>
				<li><strong>transform_mongo:</strong> Responsible for interacting with the MongoDB Atlas database, this service retrieves posts and comments, transforming the data into RDF format for later upload to Fuseki.</li>
				<li><strong>associate_things:</strong> This service connects posts/comments with birds/locations. It processes data from various datasets, establishing the necessary links between them.</li>
				<li><strong>data_provider:</strong> This service supplies the frontend with filtered and relevant information, ensuring that users receive high-quality, meaningful data for their analysis.</li>
			</ul>


		</section>


		<section id="api-architecture">
			<h2>API Architecture</h2>
			<p>MiRT employs RESTful APIs to manage data interactions for comments and posts, allowing for CRUD operations on both entities.</p>

			<!-- Comments API Section -->
			<h3>Comments API</h3>
			<ul>
				<li><strong>GET /comments/</strong> - Retrieves a list of all comments. (Endpoint: <code>comments_list</code>)</li>
				<li><strong>POST /comments/</strong> - Creates a new comment. (Endpoint: <code>comments_create</code>)</li>
				<li><strong>GET /comments/{_id}/</strong> - Retrieves a specific comment by ID. (Endpoint: <code>comments_read</code>)</li>
				<li><strong>PUT /comments/{_id}/</strong> - Updates an existing comment entirely. (Endpoint: <code>comments_update</code>)</li>
				<li><strong>PATCH /comments/{_id}/</strong> - Partially updates specific fields of a comment. (Endpoint: <code>comments_partial_update</code>)</li>
				<li><strong>DELETE /comments/{_id}/</strong> - Deletes a comment by ID. (Endpoint: <code>comments_delete</code>)</li>
				<li><strong>GET /comments/{post_id}/</strong> - Retrieves comments related to a specific post. (Endpoint: <code>comments_read</code>)</li>
				<li><strong>POST /comments/{post_id}/</strong> - Creates a comment for a specific post. (Endpoint: <code>comments_create</code>)</li>
			</ul>

			<!-- Posts API Section -->
			<h3>Posts API</h3>
			<ul>
				<li><strong>GET /posts/</strong> - Retrieves a list of all posts. (Endpoint: <code>posts_list</code>)</li>
				<li><strong>POST /posts/</strong> - Creates a new post. (Endpoint: <code>posts_create</code>)</li>
				<li><strong>GET /posts/{_id}/</strong> - Retrieves a specific post by ID. (Endpoint: <code>posts_read</code>)</li>
				<li><strong>PUT /posts/{_id}/</strong> - Updates an existing post entirely. (Endpoint: <code>posts_update</code>)</li>
				<li><strong>PATCH /posts/{_id}/</strong> - Partially updates specific fields of a post. (Endpoint: <code>posts_partial_update</code>)</li>
				<li><strong>DELETE /posts/{_id}/</strong> - Deletes a post by ID. (Endpoint: <code>posts_delete</code>)</li>
			</ul>

			<p>All API endpoints return data in JSON format. For operations that modify resources (POST, PUT, PATCH), ensure the request body follows the expected schema, including required fields such as <code>title</code>, <code>content</code>, or <code>text</code> where applicable.</p>
		</section>


        <section id="rdf-models">
            <h2>RDF-Based Knowledge Models</h2>
            <p>MiRT leverages RDF to structure and semantically enrich migration data. We utilize vocabularies from DBpedia and Wikidata to ensure data interoperability:</p>
            <ul>
                <li><strong>Ontology Design:</strong> This defines the core classes of our ontology, such as Bird, <em>Location</em>, <em>Post</em>, and <em>Comment</em>, along with their relationships and properties. It serves as the backbone of the entire dataset, providing a structured framework upon which instances are built.</li>
                <li><strong>SPARQL Endpoint:</strong> A crucial access point for querying the RDF database, allowing users to execute a wide range of queries and retrieve relevant information based on their search criteria.</li>
				<li><strong>Data Enrichment:</strong>Since data is never enough, we leverage <em>DBpedia</em> to obtain additional information. As a reliable and well-structured source, it provides accurate and contextually relevant results that enhance our dataset.</li>
				<li><strong>SHACL Validation:</strong>Beyond offering an endpoint that delivers valuable information, robust validation is essential. This is where <em>SHACL</em> comes into play, enforcing constraints on our data to ensure consistency, integrity, and reliability in our queries.</li>
            </ul>
        </section>

        <section id="external-sources">
            <h2>Use of External Data Sources</h2>
            <p>MiRT integrates external data from:</p>
            <ul>
                <li><strong>DBpedia:</strong> To further enrich our RDF database, we leveraged the DBpedia endpoint. By executing SPARQL queries, we retrieved a wide range of valuable information about birds, locations, and more. This allowed us to enhance the dataset with relevant details, ultimately helping us build a fully functional and user-friendly application.</li>
                <li><strong>Mockaroo:</strong> For generating synthetic data during testing phases.</li>
                <li><strong>free-for.dev APIs:</strong> To accurately present users with the migrations they were searching for, we needed to obtain geospatial information based on the referenced locations. To streamline this process and enhance the user experience, we utilized specialized tools that allowed us to efficiently integrate and process this data within the application.</li>
            </ul>
            <p>Example SPARQL Query:</p>
            <pre><code>SELECT ?bird ?label ?abstract ?scientificName ?family ?species ?genus ?thumbnail ?url WHERE {
				?bird rdf:type dbo:Bird ;
					rdfs:label ?label ;
					dbo:abstract ?abstract .
				
				OPTIONAL { ?bird dbp:scientificName ?scientificName . }
				OPTIONAL { ?bird dbp:family ?family . }
				OPTIONAL { ?bird dbp:species ?species . }
				OPTIONAL { ?bird dbp:genus ?genus . }
				OPTIONAL { ?bird dbo:thumbnail ?thumbnail . }
				OPTIONAL { ?bird dbp:url ?url . }
				
				FILTER (lang(?label) = "en")
				FILTER (lang(?abstract) = "en")
			}
			LIMIT 5000</code></pre>
        </section>

		<section id="linked-data">
			<h2>Linked Data Conformance</h2>
			<p>MiRT (Migration Reporting Tool) follows the core principles of Linked Data to ensure seamless integration, rich contextualization, and semantic interoperability of migration-related information. By leveraging RDF (Resource Description Framework) and SPARQL queries, MiRT enhances the migration data with external knowledge bases like DBpedia</p>

			<h3>Principles of Linked Data</h3>
			<ul>
				<li><strong>Use URIs to Identify Resources:</strong> Each migration event, entity (such as birds, humans or extraterrestrial beings), location, and metadata element within MiRT is assigned a unique Uniform Resource Identifier (URI). This ensures that each data element can be referenced unambiguously across systems.</li>
				<li><strong>Make URIs Dereferenceable:</strong> This means that when a user or system queries the URI via HTTP, it returns meaningful information about the resource. This is achieved through the integration of external SPARQL endpoints that offer real-time, rich data about the migration events and entities, directly enhancing the contextual data available for analysis.

				For example, by querying a migration event URI, MiRT retrieves not only the event details (species, location, date) but also contextual information about the species and location, pulling in data from sources like Wikidata</li>
				<li><strong>Provide Data in Standard RDF Formats:</strong> All data in MiRT is represented in the RDF format. This ensures that migration events, locations, and entities are structured in a way that makes them easy to query, analyze, and integrate with other data sources. By adhering to RDF, MiRT provides an open, standardized format that facilitates integration and reuse.</li>
				<li><strong>Include Links to External Datasets:</strong> MiRT enhances the semantic richness of migration data by linking internal data to external knowledge bases like Wikidata. This linkage provides additional context for species, locations, and migration patterns.</li>
			</ul>

			<h3>Implementation in MiRT</h3>
			<h4>RDF Data Model</h4>
			<p>MiRT's RDF data model defines key classes and properties for representing migration events:</p>
			<ul>
				<li><strong>Classes:</strong>
					<ul>
						<li><code>:Migration</code> (with properties like <code>:date</code>, <code>:species</code>, <code>:location</code>)</li>
						<li><code>:LivingBeing</code> (covering individuals like <code>:Bird</code>, <code>:Human</code>, <code>:Extraterrestrial</code>)</li>
						<li><code>:Location</code> (linked to geospatial identifiers)</li>
					</ul>
				</li>
				<li><strong>Properties:</strong> <code>:hasSpecies</code>, <code>:hasLocation</code>, <code>:observedAt</code>, <code>:isMentionedIn</code></li>
			</ul>

			<h4>Example RDF Triples</h4>
			<pre><code>
				PREFIX :<http://www.semanticweb.org/anton/ontologies/2025/0/mirt_ont/>
				PREFIX mirt_ont:<http://www.semanticweb.org/anton/ontologies/2025/0/mirt_ont#>
				PREFIX owl:<http://www.w3.org/2002/07/owl#>
				PREFIX rdf:<http://www.w3.org/1999/02/22-rdf-syntax-ns#>
				PREFIX rdfs:<http://www.w3.org/2000/01/rdf-schema#>
				PREFIX xml:<http://www.w3.org/XML/1998/namespace>
				PREFIX xsd:<http://www.w3.org/2001/XMLSchema#>
				<http://dbpedia.org/resource/Caribbean_dove>
				rdf:type                    mirt_ont:Bird;
				rdfs:label                  "Caribbean dove";
				mirt_ont:hasDescription     "The Caribbean dove (Leptotila jamaicensis) is a species of bird in the family Columbidae. It is found in Belize, the Cayman Islands, Colombia (San Andrés island), Honduras (Bay Islands), Jamaica, and Mexico (Yucatán Peninsula). It has been introduced to the Bahamas.";
				mirt_ont:hasFamily          "Unknown";
				mirt_ont:hasGenus           "Leptotila";
				mirt_ont:hasScientificName  "Unknown";
				mirt_ont:hasSpecies         "Caribbean dove" , "jamaicensis";
				mirt_ont:hasThumbnail       "http://commons.wikimedia.org/wiki/Special:FilePath/VioletDove2.jpg?width=300";
				mirt_ont:hasUrl             "https://www.xeno-canto.org/species/Leptotila-jamaicensis" .</code></pre>

			<h3>SPARQL Query Example</h3>
			<pre><code>PREFIX :<http://www.semanticweb.org/anton/ontologies/2025/0/mirt_ont/>
				PREFIX mirt_ont:<http://www.semanticweb.org/anton/ontologies/2025/0/mirt_ont#>
				PREFIX owl:<http://www.w3.org/2002/07/owl#>
				PREFIX rdf:<http://www.w3.org/1999/02/22-rdf-syntax-ns#>
				PREFIX rdfs:<http://www.w3.org/2000/01/rdf-schema#>
				PREFIX xml:<http://www.w3.org/XML/1998/namespace>
				PREFIX xsd:<http://www.w3.org/2001/XMLSchema#>

		SELECT ?place ?label ?abstract ?latitude ?longitude
        WHERE {
            ?place rdf:type ?type ;
                rdfs:label ?label ;
                dbo:abstract ?abstract ;
                geo:lat ?latitude ;
                geo:long ?longitude .

            FILTER (lang(?label) = "en")
            FILTER (lang(?abstract) = "en")

            FILTER (?type IN (dbo:City, dbo:Country, dbo:Region, dbo:Capital))

            FILTER (strlen(?abstract) > 200)  
        }
        ORDER BY DESC(strlen(?abstract))  
        LIMIT 250</code></pre>

			<h3>Benefits of Linked Data Conformance</h3>
			<ul>
				<li><strong>Enhanced Data Interoperability:</strong> Seamless integration with external knowledge bases like DBpedia, Wikidata, and GeoNames.</li>
				<li><strong>Improved Data Discoverability:</strong> Publicly dereferenceable URIs enable easy data discovery and linkage by third-party applications.</li>
				<li><strong>Richer Contextualization:</strong> Linking to external datasets provides additional context, enriching migration event analyses.</li>
				<li><strong>Flexible Data Querying:</strong> SPARQL enables complex queries to uncover patterns, trends, and insights.</li>
			</ul>

			<h3>Linked Data Visualization</h3>
			<p>The following diagram illustrates how MiRT connects internal data with external datasets using Linked Data principles:</p>
			<img src="resources/linked_data_architecture.png" alt="Linked Data Architecture">

			<h3>Conclusion</h3>
			<p>By adopting Linked Data principles, MiRT not only enhances the semantic richness and interoperability of its migration-related data but also positions itself as a valuable contributor to the broader Linked Open Data (LOD) ecosystem. This approach fosters collaborative research, data sharing, and comprehensive migration analysis across various domains.</p>
		</section>

    </main>

    <footer>
        <p>&copy; 2025 MiRT Development Team. All rights reserved.</p>
    </footer>
</body>
</html>
